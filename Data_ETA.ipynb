{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from suicide_squad.DataUtils import *\n",
    "from suicide_squad.DataProcessing import *\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering,BertConfig\n",
    "from transformers.tokenization_bert import BasicTokenizer, whitespace_tokenize\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import logging\n",
    "from ast import literal_eval\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('transformers.tokenization_utils').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(text):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text,max_length=800))\n",
    "\n",
    "def to_features(target,issues):\n",
    "    \n",
    "    target['text']='[CLS] '+target['question_text']+' [SEP] '+target['doc_text']+' [SEP]'\n",
    "    \n",
    "    doc_text_id_list=tokenizer.encode(target.doc_text)[1:-1]\n",
    "    answer_id_list=tokenizer.encode(target.answer_text)[1:-1]\n",
    "    question_id_list=tokenizer.encode(target.question_text)[1:-1]\n",
    "    \n",
    "    doc_text_id=' '.join([str(item) for item in doc_text_id_list])\n",
    "    answer_id=' '.join([str(item) for item in answer_id_list])\n",
    "    question_id=' '.join([str(item) for item in question_id_list])\n",
    "    \n",
    "    if((target.answer_text) and (target.question_text) and (target.doc_text)):\n",
    "        try:\n",
    "            matches=[match for match in re.finditer(answer_id, doc_text_id)]\n",
    "            if(matches):\n",
    "                start=matches[0].start()\n",
    "                end=matches[0].end()\n",
    "                target['start_label']=len(doc_text_id[:start].split(' '))+len(question_id_list)+1\n",
    "                target['end_label']=len(answer_id_list)+target['start_label']\n",
    "                answer_id_extracted=tokenizer.encode(target.question_text,target.doc_text)[target['start_label']:target['end_label']]\n",
    "                target['extracted_answer']=tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_id_extracted))\n",
    "                target['answers_match']=(answer_id==' '.join([str(item) for item in answer_id_extracted]))\n",
    "            else:\n",
    "                issues.append(target.name)\n",
    "                target['start_label']=0\n",
    "                target['end_label']=0\n",
    "        except Exception as e:\n",
    "            logging.warning(e)\n",
    "            target['start_label']=0\n",
    "            target['end_label']=0\n",
    "        \n",
    "        target['total_tokens']=len(doc_text_id.split(' '))+len(question_id.split(' '))+3\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Files\n",
      "Converting Files to CSV\n",
      "Finished Downloading and Converting to CSV\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['D:\\\\GitHub\\\\Suicide-SQuAD\\\\data\\\\original\\\\train-v1.1.csv',\n",
       " 'D:\\\\GitHub\\\\Suicide-SQuAD\\\\data\\\\original\\\\dev-v1.1.csv',\n",
       " 'D:\\\\GitHub\\\\Suicide-SQuAD\\\\data\\\\original\\\\train-v2.0.csv',\n",
       " 'D:\\\\GitHub\\\\Suicide-SQuAD\\\\data\\\\original\\\\dev-v2.0.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_to_csv(files,download_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Files\n",
      "Downloading Files\n",
      "Converting Files to CSV\n",
      "Finished Downloading and Converting to CSV\n",
      "Converting Files to DataFrame\n",
      "Finished Downloading and Converting to DataFrame\n"
     ]
    }
   ],
   "source": [
    "squad_dfs=squad_to_df(files,download_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_issues={}\n",
    "selected_columns=['qa_id','start_label','end_label','text','total_tokens']\n",
    "squad_final={}\n",
    "for k,v in squad_dfs.items():\n",
    "    \n",
    "    print('Preprocessing: ',k)\n",
    "    issues=[]\n",
    "    temp=v.dropna(subset=['answer_text'])\n",
    "    temp=temp[~temp.is_impossible]\n",
    "    if('dev' in k):\n",
    "        count_starts=temp['answer_text'].apply(literal_eval).apply(lambda x:len(set(x)))\n",
    "        temp_new=pd.DataFrame({col:np.repeat(temp[col].values, count_starts) for col in temp.columns.drop('answer_text')})\n",
    "        answers=temp['answer_text'].apply(literal_eval).apply(lambda x:np.unique(x)).values\n",
    "        answers_all=[]\n",
    "        for item_a in answers:\n",
    "            for item_b in item_a:\n",
    "                answers_all.append(item_b)\n",
    "        temp_new['answer_text']=answers_all\n",
    "        temp=temp_new\n",
    "    features=temp.apply(lambda x:to_features(x,issues),axis=1)\n",
    "\n",
    "    all_issues[k]=temp[temp.index.isin(issues)]\n",
    "    temp[temp.index.isin(issues)].to_csv('.//data/processed//'+k+'_issues.csv')\n",
    "    features.dropna(how='all',inplace=True)\n",
    "    final=features[~features.index.isin(issues)]\n",
    "    squad_final[k]=final\n",
    "    final.to_csv('.//data/processed//'+k+'_bert_preprocessed.csv')\n",
    "    print('Finished Preprocessing: ',k)\n",
    "    clear_output(wait=True)\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "for k,v in squad_final.items():\n",
    "    spans=[]\n",
    "    print('Processing: ',k)\n",
    "    name=k.replace('.','_').replace('-','_')\n",
    "    if('dev' in k):\n",
    "        qa_id_span_map={}\n",
    "        for g_label,g in v.groupby(['qa_id']):\n",
    "            span=[list(range(*item)) for item in list(zip(g.start_label,g.end_label+1))]\n",
    "            qa_id_span_map[g_label]=str(list(np.unique(np.hstack(span))))\n",
    "        temp=v.copy()\n",
    "        temp['span_label']=v['qa_id'].map(qa_id_span_map)\n",
    "        temp=temp.drop_duplicates('qa_id')\n",
    "        temp.to_csv('.//data/processed//'+name+'_bert_ready.csv')\n",
    "        print('finished processing ',k)\n",
    "        clear_output(wait=True)\n",
    "    else:\n",
    "        temp=v.copy()\n",
    "        span=[list(range(*item)) for item in list(zip(v.start_label,v.end_label+1))]\n",
    "        temp.loc[:,'span_label']=span\n",
    "        temp.to_csv('.//data/processed//'+name+'_bert_ready.csv')\n",
    "        print('finished processing ',k)\n",
    "        clear_output(wait=True)        \n",
    "        \n",
    "clear_output(wait=True)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " - doc_text: Document text\n",
    " - qa_id: unique id for the question\n",
    " - question_text:  A question.\n",
    " - is_impossible: It is not possible to answer the question\n",
    " - answer_text: Possible answer to the question\n",
    " - answer_start: index for where the answer begings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:devenv] *",
   "language": "python",
   "name": "conda-env-devenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
